{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwGbXYfJf5JrGZlphLp4P9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UWObV9zNZc9W","executionInfo":{"status":"ok","timestamp":1729838901907,"user_tz":-330,"elapsed":9906,"user":{"displayName":"ankit singh","userId":"04293018571785188855"}},"outputId":"85688e03-8b6a-44e3-d148-56c10028f23f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"]}],"source":["! pip install nltk"]},{"cell_type":"code","source":["corpus = \"\"\"Hello, my name is ankit singh.\n","i like eating mangoes! it's yummyyy.\n","\"\"\"\n"],"metadata":{"id":"DCZZTisVbpYd","executionInfo":{"status":"ok","timestamp":1729840800292,"user_tz":-330,"elapsed":497,"user":{"displayName":"ankit singh","userId":"04293018571785188855"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["### tokenization 1 - sent_tokenize\n","\n","converting paragraph into sentence\n","\n","Punkt is a pre-trained tokenizer model provided by NLTK (Natural Language Toolkit) that helps break text into sentences. It uses unsupervised machine learning to identify sentence boundaries based on language-specific patterns, without requiring rule-based instructions like punctuation marks. This makes it robust in handling abbreviations, special cases, and various punctuation styles across different languages.\n","\n","In NLTK, punkt is often used with functions like sent_tokenize, allowing you to segment text into sentences accurately and efficiently for NLP tasks like text summarization, information extraction, and translation."],"metadata":{"id":"0x-5vrOrcSuk"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNkBM97CcZWM","executionInfo":{"status":"ok","timestamp":1729840802807,"user_tz":-330,"elapsed":493,"user":{"displayName":"ankit singh","userId":"04293018571785188855"}},"outputId":"54bd3b55-6a9d-405f-d349-1c90a4ff6200"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize ## used to convert paragraphs into sentences\n","\n","documents = sent_tokenize(corpus)\n","documents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XA987qtfc0xj","executionInfo":{"status":"ok","timestamp":1729840804831,"user_tz":-330,"elapsed":644,"user":{"displayName":"ankit singh","userId":"04293018571785188855"}},"outputId":"e816a737-5664-424c-a45e-b78a66cb0dbd"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello, my name is ankit singh.', 'i like eating mangoes!', \"it's yummyyy.\"]"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["for sentences in documents:\n","  print(sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_IjGmRWeP26","executionInfo":{"status":"ok","timestamp":1729840808977,"user_tz":-330,"elapsed":485,"user":{"displayName":"ankit singh","userId":"04293018571785188855"}},"outputId":"148a5625-8107-4e66-ccb0-41a103fd356c"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, my name is ankit singh.\n","i like eating mangoes!\n","it's yummyyy.\n"]}]},{"cell_type":"markdown","source":["## tokenization 2 - word_tokenize\n","\n","#### using this, we can convert -\n","    paragraph --> words\n","    sentences --> words\n"],"metadata":{"id":"q20GeX2lfJ9J"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","\n","words = word_tokenize(corpus)\n","words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qm7WdScofegn","executionInfo":{"status":"ok","timestamp":1729840818016,"user_tz":-330,"elapsed":575,"user":{"displayName":"ankit singh","userId":"04293018571785188855"}},"outputId":"4bc8437e-0c80-4754-bb13-f5d494ee6bbe"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," ',',\n"," 'my',\n"," 'name',\n"," 'is',\n"," 'ankit',\n"," 'singh',\n"," '.',\n"," 'i',\n"," 'like',\n"," 'eating',\n"," 'mangoes',\n"," '!',\n"," 'it',\n"," \"'s\",\n"," 'yummyyy',\n"," '.']"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["words2 = word_tokenize(documents[1])\n","words2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUo_9HeOfrqx","executionInfo":{"status":"ok","timestamp":1729840827485,"user_tz":-330,"elapsed":492,"user":{"displayName":"ankit singh","userId":"04293018571785188855"}},"outputId":"50aa0898-1ac4-4b9a-beeb-92af4ec175b9"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i', 'like', 'eating', 'mangoes', '!']"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## Tokenization 3 - wordpunct_tokenize\n","\n","The main difference between word_tokenize and wordpunct_tokenize in NLTK lies in how they handle punctuation:\n","\n","1.\n","word_tokenize is a standard tokenizer in NLTK that splits text into words, handling punctuation according to more natural language processing conventions.\n","It uses TreebankWordTokenizer under the hood, which separates words from punctuation in a structured way. For example, it splits contractions (like \"can't\" to [\"ca\", \"n't\"]) but keeps abbreviations intact.\n","\n","2.\n","wordpunct_tokenize is a simpler tokenizer that splits text into words and punctuation based purely on whitespace and punctuation boundaries.\n","It will separate every punctuation mark, including contractions and abbreviations, more aggressively. For example, \"I'm\" becomes [\"I\", \"'\", \"m\"], and \"N.L.T.K.\" becomes [\"N\", \".\", \"L\", \".\", \"T\", \".\", \"K\", \".\"].\n","This tokenizer is useful if you need raw tokens without NLP-specific rules, especially if you are looking to analyze punctuation separately.\n","\n","### Summary\n","* word_tokenize: More NLP-friendly, retains contractions and some abbreviations as single tokens.\n","* wordpunct_tokenize: Breaks text purely by whitespace and punctuation boundaries, separating out every punctuation mark, making it more granular."],"metadata":{"id":"1UkTZvmYgbqH"}},{"cell_type":"code","source":["from nltk.tokenize import wordpunct_tokenize\n","\n","wordPunct = wordpunct_tokenize(corpus)\n","wordPunct"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pz1qltFaglOm","executionInfo":{"status":"ok","timestamp":1729840834651,"user_tz":-330,"elapsed":488,"user":{"displayName":"ankit singh","userId":"04293018571785188855"}},"outputId":"ec81be62-074a-42f3-e7b5-621840ff3395"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," ',',\n"," 'my',\n"," 'name',\n"," 'is',\n"," 'ankit',\n"," 'singh',\n"," '.',\n"," 'i',\n"," 'like',\n"," 'eating',\n"," 'mangoes',\n"," '!',\n"," 'it',\n"," \"'\",\n"," 's',\n"," 'yummyyy',\n"," '.']"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["## Tokenization 4 - TreeBankTokenizer\n","\n","in TreeBankTokenizer , FullStop (.) that are between the sentences are not treated as a different word but the last fullstop is treated as a different word\n","\n","Both word_tokenize and TreebankWordTokenizer are tokenization functions in NLTK, but they work slightly differently, mainly in their handling of punctuation and language-specific rules.\n","\n","1.\n","word_tokenize is a generic tokenization function in NLTK.\n","Internally, it uses the TreebankWordTokenizer, but with additional steps. For instance, it includes PunktSentenceTokenizer to first break the text into sentences (if the text contains multiple sentences).\n","It is flexible, working well for simple cases but sometimes less precise with punctuation compared to TreebankWordTokenizer directly.\n","\n","2.\n","TreebankWordTokenizer is a tokenizer modeled after the Penn Treebank, which is a corpus used for training parsers in NLP.\n","This tokenizer is more specific and precise in handling punctuation and token boundaries based on standard Treebank tokenization rules.\n","It splits contractions (like \"can't\" to \"ca n't\") and handles punctuation separately, so itâ€™s especially useful for preparing text for tasks requiring fine-grained tokenization like parsing and tagging.\n"],"metadata":{"id":"gyLUUyUyiLJ1"}},{"cell_type":"code","source":["from nltk.tokenize import TreebankWordTokenizer\n","\n","tokenizer = TreebankWordTokenizer()\n","tokenizer.tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KWBgcYtiZSV","executionInfo":{"status":"ok","timestamp":1729840844419,"user_tz":-330,"elapsed":544,"user":{"displayName":"ankit singh","userId":"04293018571785188855"}},"outputId":"600631ab-8118-44c5-c69a-bf2bc243fa8f"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," ',',\n"," 'my',\n"," 'name',\n"," 'is',\n"," 'ankit',\n"," 'singh.',\n"," 'i',\n"," 'like',\n"," 'eating',\n"," 'mangoes',\n"," '!',\n"," 'it',\n"," \"'s\",\n"," 'yummyyy',\n"," '.']"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":[],"metadata":{"id":"0c_gcEjNifum"},"execution_count":null,"outputs":[]}]}